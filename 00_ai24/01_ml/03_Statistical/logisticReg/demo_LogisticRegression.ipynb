{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Lab Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "The _logistic regression_ can be derived in many forms.  \n",
    "We'll illustrate 2 of them.\n",
    "\n",
    "### Derivation 001\n",
    "\n",
    "One intuitive path is saying that we're after calculating the probability: $p \\left( y = 1 \\mid \\boldsymbol{x} \\right)$.  \n",
    "Since it is a probability function is must obey some rules. The first one being in the range $\\left[ 0, 1 \\right]$.  \n",
    "\n",
    "A function which maps $\\left( -\\infty, \\infty \\right) \\to \\left[0, 1 \\right]$ is the [Sigmoid Function](https://en.wikipedia.org/wiki/Sigmoid_function): $\\sigma \\left( z \\right) = \\frac{1}{1 + \\exp \\left( z \\right)}$.\n",
    "\n",
    "So now we can say that: $p \\left( y = 1 \\mid \\boldsymbol{x} \\right) = \\sigma \\left( {z}_{i} \\right)$.  \n",
    "Now the problem is modeling the parameter ${z}_{i}$. In which in a linear case will be modeled as ${z}_{i} = \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i}$.  \n",
    "Namely by a linear model, which in the choice of the Sigmoid Function means the objective function is Convex in $\\boldsymbol{w}_{i}$ and $b$:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Exam_pass_logistic_curve.svg/640px-Exam_pass_logistic_curve.svg.png)\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Actually it is convex only if the problem is not linear separable.\n",
    "\n",
    "If we expand the above to multi class we'll get the [Softmax Function](https://en.wikipedia.org/wiki/Softmax_function) as in slides.\n",
    "\n",
    "### Derivation 002\n",
    "\n",
    "By _Bayes Theorem_ for the $L$ classes model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) & = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\right) } && \\text{} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ \\sum_{j = 1}^{L} p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right) } && \\text{Expending by law total probability} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) + p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right) } && \\text{Expending by law total probability} \\\\\n",
    "& = \\frac{ 1 }{ 1 + \\frac{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)} } && \\text{Dividing by $p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)$} \\\\\n",
    "& = \\frac{ 1 }{ 1 + {e}^{\\log \\frac{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}} } && \\text{for $x \\in \\left[ 0, \\infty \\right) \\Rightarrow x = \\exp \\log x $} \\\\\n",
    "& = \\frac{ 1 }{ 1 + {e}^{-\\log \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right) }} } && \\text{$\\log x = - \\log \\frac{1}{x}$} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, if we model the log of likelihood ratio of the ${L}_{i}$ label with a linear model:\n",
    "\n",
    "$$ \\log \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right) } = \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} $$\n",
    "\n",
    "So we get:\n",
    "\n",
    "$$ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{1}{ 1 + {e}^{- \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right)} } $$\n",
    "\n",
    "Yet, since $1 = {e}^{- \\log \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}}$ the above can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{1}{ 1 + {e}^{- \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right)} }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Derivation 003\n",
    "\n",
    "By _Bayes Theorem_ for the $L$ classes model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) & = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\right) } && \\text{} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ \\sum_{j = 1}^{L} p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right) } && \\text{Expending by law total probability} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right) + \\sum_{j \\neq k} p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right) } && \\text{} \\\\\n",
    "& = \\frac{ \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)} }{ 1 + \\sum_{j \\neq k} \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)} } && \\text{Dividing by $p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)$} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As in above, we may model the Log Likelihood Ratio by a linear function of $\\boldsymbol{x}$ then we'll get:\n",
    "\n",
    "$$ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{ \\exp{\\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} \\right)} }{ 1 + \\sum_{j \\neq k} \\exp{\\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} \\right)}} $$\n",
    "\n",
    "Since $1 = \\exp{ \\left( \\log{ \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)} } \\right)}$ we can write:\n",
    "\n",
    "$$ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{ \\exp{\\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} \\right)} }{ \\sum_{j} \\exp{\\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} \\right)}} $$\n",
    "\n",
    "### Derivation 004\n",
    "\n",
    "Given $L$ classes, we can chose a reference class: ${L}_{k}$. Then define the linear model of the log likelihood ratio compared to it:\n",
    "\n",
    "$$ \\log{ \\left( \\frac{ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) }{ p \\left( {y} = {L}_{k} \\mid \\boldsymbol{x} \\right) } \\right) } = \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} $$\n",
    "\n",
    "By definition $p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) }$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 - p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) & = \\sum_{j \\neq k} p \\left( y = {L}_{j} \\mid \\boldsymbol{x} \\right) && \\text{} \\\\\n",
    "& = \\sum_{j \\neq k} p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) } && \\text{Since $p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) }$} \\\\\n",
    "& = p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) } && \\text{} \\\\\n",
    "& \\Rightarrow p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) = \\frac{1}{1 + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} \\\\\n",
    "& \\Rightarrow p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{1 + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} && \\text{}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $1 = \\exp{\\left( \\log{ \\frac{ p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) }{ p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) } } \\right)}$ we can write:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) & = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{1 + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} \\\\\n",
    "& = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{\\exp{ \\left( \\boldsymbol{w}_{k}^{T} \\boldsymbol{x} + {b}_{k} \\right) } + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} \\\\\n",
    "& = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{ \\sum_{j} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Summary\n",
    "\n",
    "While there are many way to derive the logistic regression (for instance, also by assuming Binomial Distribution), the main motivation is its numerical properties.  \n",
    "Namely being convex with easy to calculate gradient.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The first \"Deep Learning\" model were actually chaining many logistic regression layers.\n",
    "* <font color='brown'>(**#**)</font> Most classification layers in Deep Learning models are basically Logistic Regression.\n",
    "* <font color='brown'>(**#**)</font> The concept of Logistic Regression can also be used as pure regression for continuous data bounded in the range $\\left[ a, b \\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.33%\n",
      "Confusion Matrix:\n",
      "[[137  10]\n",
      " [ 25 128]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_clusters_per_class=1, flip_y=0.01, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Optional: Plot decision boundary (requires additional code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this snippet, predict_proba is used instead of predict to obtain the probabilities of the class labels. The log_loss function from sklearn.metrics then computes the cross-entropy loss for these predictions against the actual labels. A lower cross-entropy loss indicates a model that predicts class labels with higher confidence and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.2992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_probs = model.predict_proba(X_test)\n",
    "\n",
    "# Compute the cross-entropy loss\n",
    "loss = log_loss(y_test, y_pred_probs)\n",
    "\n",
    "print(f\"Cross-Entropy Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
