{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.CrossEntropyLoss\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "`nn.CrossEntropyLoss` is a PyTorch loss function commonly used in classification problems. It combines `nn.LogSoftmax` and `nn.NLLLoss` (negative log-likelihood loss) in one single class. This loss function is used when training a classification model that outputs raw, unnormalized scores for each class.\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "#### 1. **Purpose and Usage**\n",
    "\n",
    "`nn.CrossEntropyLoss` is designed to measure the performance of a classification model whose output is a probability distribution across classes. It is suitable for multi-class classification problems where the model output is a vector of raw scores (also called logits) for each class.\n",
    "\n",
    "The loss function computes the cross-entropy between the true labels and the predicted probabilities. Cross-entropy measures the difference between two probability distributions - the true distribution (one-hot encoded vector of true labels) and the predicted distribution (output probabilities from the model).\n",
    "\n",
    "#### 2. **Mathematical Formulation**\n",
    "\n",
    "The cross-entropy loss for a single instance can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{c=1}^{C} y_{c} \\log(\\hat{p}_{c})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( C \\) is the number of classes.\n",
    "- \\( y_{c} \\) is a binary indicator (0 or 1) if class label \\( c \\) is the correct classification for the current observation.\n",
    "- \\( \\hat{p}_{c} \\) is the predicted probability for class \\( c \\).\n",
    "\n",
    "In PyTorch, the function internally applies `nn.LogSoftmax` to the logits to get log-probabilities and then applies `nn.NLLLoss` to compute the negative log likelihood.\n",
    "\n",
    "#### 3. **Implementation in PyTorch**\n",
    "\n",
    "Here is an example of how to use `nn.CrossEntropyLoss` in a simple neural network:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1310322284698486\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sample data\n",
    "inputs = torch.randn(10, 5)  # Batch size of 10, feature size of 5\n",
    "targets = torch.randint(0, 3, (10,))  # Random target labels (3 classes)\n",
    "\n",
    "# Define a simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(outputs, targets)\n",
    "\n",
    "# Backward pass and optimization\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Real-World Example**\n",
    "\n",
    "In a real-world scenario, consider a neural network designed to classify images of handwritten digits (0-9) from the MNIST dataset. Here, `nn.CrossEntropyLoss` would be ideal since the task involves multi-class classification, and the model needs to output a probability distribution over 10 classes (one for each digit).\n",
    "\n",
    "By using `nn.CrossEntropyLoss`, you can ensure that the model learns to assign high probabilities to the correct digit labels and low probabilities to the incorrect ones, effectively minimizing the loss during training.\n",
    "\n",
    "---\n",
    "\n",
    "This loss function is integral to training models in classification tasks, ensuring that the predictions are as close as possible to the actual labels, thus improving the model's accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}